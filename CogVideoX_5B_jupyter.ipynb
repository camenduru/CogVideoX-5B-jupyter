{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/CogVideoX-5B-jupyter/blob/main/CogVideoX_5B_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "!pip install xformers==0.0.27.post2 diffusers==0.30.1 accelerate==0.33.0 sentencepiece==0.2.0 moviepy==1.0.3\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/scheduler/scheduler_config.json -d /content/CogVideoX-5b/scheduler/ -o scheduler_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/text_encoder/config.json -d /content/CogVideoX-5b/text_encoder/ -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/resolve/main/text_encoder/model-00001-of-00002.safetensors -d /content/CogVideoX-5b/text_encoder/ -o model-00001-of-00002.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/resolve/main/text_encoder/model-00002-of-00002.safetensors -d /content/CogVideoX-5b/text_encoder/ -o model-00002-of-00002.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/text_encoder/model.safetensors.index.json -d /content/CogVideoX-5b/text_encoder/ -o model.safetensors.index.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/tokenizer/added_tokens.json -d /content/CogVideoX-5b/tokenizer -o /added_tokens.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/tokenizer/special_tokens_map.json -d /content/CogVideoX-5b/tokenizer/ -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/resolve/main/tokenizer/spiece.model -d /content/CogVideoX-5b/tokenizer/ -o spiece.model\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/tokenizer/tokenizer_config.json -d /content/CogVideoX-5b/tokenizer/ -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/transformer/config.json -d /content/CogVideoX-5b/transformer/ -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/resolve/main/transformer/diffusion_pytorch_model-00001-of-00002.safetensors -d /content/CogVideoX-5b/transformer/ -o diffusion_pytorch_model-00001-of-00002.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/resolve/main/transformer/diffusion_pytorch_model-00002-of-00002.safetensors -d /content/CogVideoX-5b/transformer/ -o diffusion_pytorch_model-00002-of-00002.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/transformer/diffusion_pytorch_model.safetensors.index.json -d /content/CogVideoX-5b/transformer/ -o diffusion_pytorch_model.safetensors.index.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/vae/config.json -d /content/CogVideoX-5b/vae/ -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/resolve/main/vae/diffusion_pytorch_model.safetensors -d /content/CogVideoX-5b/vae/ -o diffusion_pytorch_model.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/configuration.json -d /content/CogVideoX-5b/ -o configuration.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/vdo/CogVideoX-5b/raw/main/model_index.json -d /content/CogVideoX-5b/ -o model_index.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from typing import Union, List\n",
        "import PIL.Image\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from diffusers.image_processor import VaeImageProcessor\n",
        "from diffusers.utils import export_to_video\n",
        "from diffusers import CogVideoXPipeline, CogVideoXDDIMScheduler,CogVideoXDPMScheduler\n",
        "import moviepy.editor as mp\n",
        "\n",
        "def convert_to_gif(video_path):\n",
        "    clip = mp.VideoFileClip(video_path)\n",
        "    clip = clip.set_fps(8)\n",
        "    clip = clip.resize(height=240)\n",
        "    gif_path = video_path.replace(\".mp4\", \".gif\")\n",
        "    clip.write_gif(gif_path, fps=8)\n",
        "    return gif_path\n",
        "\n",
        "def save_video(tensor: Union[List[np.ndarray], List[PIL.Image.Image]], fps: int = 8):\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    video_path = f\"{timestamp}.mp4\"\n",
        "    export_to_video(tensor, video_path, fps=fps)\n",
        "    return video_path\n",
        "\n",
        "pipe = CogVideoXPipeline.from_pretrained(\"/content/CogVideoX-5b\", torch_dtype=torch.float16)\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_sequential_cpu_offload()\n",
        "pipe.vae.enable_slicing()\n",
        "pipe.vae.enable_tiling()\n",
        "\n",
        "# pipe.scheduler = CogVideoXDPMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "# pipe.transformer.to(memory_format=torch.channels_last)\n",
        "# pipe.transformer = torch.compile(pipe.transformer, mode=\"max-autotune\", fullgraph=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"A golden retriever, sporting sleek black sunglasses, with its lengthy fur flowing in the breeze, sprints playfully across a rooftop terrace, recently refreshed by a light rain. The scene unfolds from a distance, the dog's energetic bounds growing larger as it approaches the camera, its tail wagging with unrestrained joy, while droplets of water glisten on the concrete behind it. The overcast sky provides a dramatic backdrop, emphasizing the vibrant golden coat of the canine as it dashes towards the viewer.\"\n",
        "seed = 0\n",
        "\n",
        "if seed == 0:\n",
        "    random.seed(int(time.time()))\n",
        "    seed = random.randint(0, 18446744073709551615)\n",
        "print(seed)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    video_pt = pipe(\n",
        "        prompt=prompt,\n",
        "        num_videos_per_prompt=1,\n",
        "        num_inference_steps=50,\n",
        "        num_frames=48,\n",
        "        use_dynamic_cfg=True,\n",
        "        output_type=\"pt\",\n",
        "        guidance_scale=7.0,\n",
        "        generator=torch.Generator(device=\"cpu\").manual_seed(seed),\n",
        "    ).frames\n",
        "\n",
        "batch_size = video_pt.shape[0]\n",
        "batch_video_frames = []\n",
        "for batch_idx in range(batch_size):\n",
        "    pt_image = video_pt[batch_idx]\n",
        "    pt_image = torch.stack([pt_image[i] for i in range(pt_image.shape[0])])\n",
        "\n",
        "    image_np = VaeImageProcessor.pt_to_numpy(pt_image)\n",
        "    image_pil = VaeImageProcessor.numpy_to_pil(image_np)\n",
        "    batch_video_frames.append(image_pil)\n",
        "\n",
        "video_path = save_video(batch_video_frames[0], fps=math.ceil((len(batch_video_frames[0]) - 1) / 6))\n",
        "gif_path = convert_to_gif(video_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
